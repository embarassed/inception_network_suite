{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, one_hot_encoding, flatten, relu, l2_regularizer, xavier_initializer\n",
    "import time\n",
    "import flags\n",
    "import data_utils\n",
    "import loss\n",
    "from InceptionLayers import InceptionStemV1, InceptionModuleV1, InceptionClassifierV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Model Constants\n",
    "def run_unit_test(use_fake_data=False, test_mode=\"classifier_auxiliary\"):\n",
    "    #Test mode can either be \"module\", \"stem\", \"classifier_auxiliary\", \"classifier_basic\"\n",
    "    fl = tf.app.flags.FLAGS\n",
    "    BATCH_SIZE = fl.batch_size\n",
    "    L2_WEIGHT = fl.l2_lambda_weight\n",
    "    \n",
    "    #(train_X, train_y), (test_X, test_y), NUM_LABELS = data_utils.load_dataset(fl.dataset)\n",
    "    \n",
    "    #TODO - toss away this NUM_LABELS when done testing\n",
    "    (train_X, train_y), (test_X, test_y), NUM_LABELS = data_utils.load_dataset(fl.dataset)\n",
    "\n",
    "    #extract a random validation set from the training set\n",
    "    validation_size = np.floor(train_X.shape[0]*fl.validation_ratio).astype(int)\n",
    "    shuf = np.random.permutation(train_X.shape[0])\n",
    "    train_X = train_X[shuf]\n",
    "    train_y = train_y[shuf]\n",
    "    validation_X, validation_y = train_X[:validation_size], train_y[:validation_size]\n",
    "    train_X, train_y = train_X[validation_size:], train_y[validation_size:]\n",
    "    \n",
    "    IMAGE_LEN = train_X.shape[1]\n",
    "    IMAGE_WID = train_X.shape[2]\n",
    "    assert IMAGE_LEN == IMAGE_WID, \"Expected square images. Got %d by %d\" % (IMAGE_LEN, IMAGE_WID)\n",
    "    IMAGE_SIZE = IMAGE_LEN\n",
    "    NUM_CHANNELS = train_X.shape[3]\n",
    "\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        tf_train_X = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "        tf_train_y = tf.placeholder(tf.int32, shape=(BATCH_SIZE,1))\n",
    "        tf_validation_X = tf.placeholder(tf.float32, shape=validation_X.shape)\n",
    "        tf_validation_y = tf.placeholder(tf.int32, shape=(validation_y.shape[0],1))\n",
    "        tf_test_X = tf.placeholder(tf.float32, shape=test_X.shape)\n",
    "        tf_test_y = tf.placeholder(tf.int32, shape=(test_y.shape[0],1))\n",
    "        \n",
    "        if test_mode == \"stem\":\n",
    "            expected_output_shape = [BATCH_SIZE, IMAGE_SIZE / 8, IMAGE_SIZE / 8, 192]\n",
    "            inception_layer = InceptionStemV1(filter_sizes=[64, 64, 192],\n",
    "                                            input_shape=tf_train_X.get_shape(),\n",
    "                                            output_shape=expected_output_shape)\n",
    "        \n",
    "        elif test_mode == \"module\":\n",
    "            expected_output_shape = [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 256]\n",
    "            inception_layer = InceptionModuleV1(dtype=tf.float32,\n",
    "                                        layer_number = 1,\n",
    "                                        input_shape = tf_train_X.get_shape().as_list(),\n",
    "                                        output_shape = expected_output_shape,\n",
    "                                        filter_sizes = [64, 96, 128, 16, 32, 32])\n",
    "        \n",
    "        elif test_mode == \"classifier_auxiliary\":\n",
    "            expected_output_shape = [BATCH_SIZE, NUM_LABELS]\n",
    "            inception_layer = InceptionClassifierV1(dtype=tf.float32,\n",
    "                                        auxiliary_weight_constant=0.3,\n",
    "                                        filter_sizes=[10,1024],\n",
    "                                        auxiliary_classifier=True,\n",
    "                                        num_labels=NUM_LABELS,\n",
    "                                        input_shape=tf_train_X.get_shape().as_list(), \n",
    "                                        output_shape=expected_output_shape)\n",
    "            \n",
    "        elif test_mode == \"classifier_basic\":\n",
    "            expected_output_shape = [BATCH_SIZE, NUM_LABELS]\n",
    "            inception_layer = InceptionClassifierV1(dtype=tf.float32,\n",
    "                                        num_labels=NUM_LABELS,\n",
    "                                        input_shape=tf_train_X.get_shape().as_list(),\n",
    "                                        output_shape=expected_output_shape)\n",
    "        \n",
    "        inception_layer.create_model()\n",
    "        global_step = tf.Variable(0)\n",
    "        \n",
    "        #set up a learning rate and learning rate decay mechanism\n",
    "        lr_calc = tf.train.exponential_decay(0.01, global_step, 100, 0.999, staircase=True)\n",
    "        lr_min = 0.0001\n",
    "        lr = tf.maximum(lr_calc, lr_min)\n",
    "        \n",
    "        #set up an l2 regulariztaion and its decay mechanism operation\n",
    "        l2_lambda_weight = tf.Variable(fl.l2_lambda_weight, dtype=tf.float32)\n",
    "        l2_lambda_decay = tf.constant(fl.l2_lambda_weight_decay, dtype=tf.float32)\n",
    "        l2_lambda_decay_op = l2_lambda_weight.assign(\n",
    "            l2_lambda_weight * l2_lambda_decay)\n",
    "        \n",
    "        #reshape the images and their labels\n",
    "        #flat_inputs = flatten(tf_train_X, scope=\"flatten_pixel_channels\")\n",
    "        one_hot_train_outputs = one_hot_encoding(tf.squeeze(tf_train_y), NUM_LABELS, on_value=1.0, off_value=0.0)\n",
    "        one_hot_validation_outputs = one_hot_encoding(tf.squeeze(tf_validation_y), NUM_LABELS, on_value=1.0, off_value=0.0)\n",
    "        one_hot_test_outputs = one_hot_encoding(tf.squeeze(tf_test_y), NUM_LABELS, on_value=1.0, off_value=0.0)\n",
    "\n",
    "        #A cheap model that tosses a fully-connected layer on to the flattened result of the 4d Tensor\n",
    "        \n",
    "        if test_mode in [\"module\", \"stem\"]:\n",
    "            \n",
    "            flattened_incept_out_size = expected_output_shape[1]*expected_output_shape[2]*expected_output_shape[3]\n",
    "            \n",
    "            w_l2 = tf.get_variable(\"w_l2\",\n",
    "                           shape=(flattened_incept_out_size, one_hot_train_outputs.get_shape()[1]),\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=xavier_initializer())\n",
    "            b_l2 = tf.get_variable(\"b_l2\",\n",
    "                           shape=(one_hot_train_outputs.get_shape()[1]),\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=tf.zeros_initializer())\n",
    "            \n",
    "            def model_with_linear_classifier(inp, training=True):\n",
    "                inception_out = inception_layer.run_model(inp)\n",
    "                flat_inputs = flatten(inception_out)\n",
    "                return tf.matmul(flat_inputs, w_l2) + b_l2\n",
    "            \n",
    "            train_out = model_with_linear_classifier(tf_train_X)\n",
    "            train_predictions = tf.nn.softmax(train_out)\n",
    "            validation_out = model_with_linear_classifier(tf_validation_X, training=False)\n",
    "            validation_predictions = tf.nn.softmax(validation_out)\n",
    "            test_out = model_with_linear_classifier(tf_test_X, training=False)\n",
    "            test_predictions = tf.nn.softmax(test_out)\n",
    "        \n",
    "        else:\n",
    "            #softmax is part of the inception layer in the classifiers\n",
    "            train_out = inception_layer.run_model(tf_train_X, training=True)\n",
    "            train_predictions = tf.nn.softmax(train_out)\n",
    "            validation_out = inception_layer.run_model(tf_validation_X, training=False)\n",
    "            validation_predictions = tf.nn.softmax(validation_out)\n",
    "            test_out = inception_layer.run_model(tf_test_X, training=False)\n",
    "            test_predictions = tf.nn.softmax(test_out)\n",
    "        \n",
    "        #separate the losses so we can compare them in the session\n",
    "        ce_loss = loss.softmax_cross_entropy_with_laplace_smoothing(train_out, one_hot_train_outputs, laplace_pseudocount=0.00001)\n",
    "        \n",
    "        #collect all the parameters in the model to do l2 regulariztion\n",
    "        regularization_parameters = inception_layer.model_parameters\n",
    "        if test_mode in [\"module\", \"stem\"]:\n",
    "            regularization_parameters.extend((w_l2, b_l2))\n",
    "        \n",
    "        reg_loss = loss.regularizer(regularization_parameters, reg_type='l2', weight_lambda=0.001)\n",
    "        \n",
    "        total_loss = tf.reduce_mean(ce_loss + reg_loss)\n",
    "        opt = tf.train.GradientDescentOptimizer(lr).minimize(total_loss, global_step=global_step)\n",
    "        \n",
    "        #we also declare this in the graph and run it in the session\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    with tf.Session(graph=g, config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    \n",
    "        sess.run(init_op)\n",
    "        total_steps = 0\n",
    "        num_epochs = 100\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            shuf = np.random.permutation(train_X.shape[0])\n",
    "            train_X = train_X[shuf]\n",
    "            train_y = train_y[shuf]\n",
    "            processed=0\n",
    "\n",
    "            while processed+BATCH_SIZE <= train_X.shape[0]:\n",
    "                batch_X = train_X[processed:processed+BATCH_SIZE]\n",
    "                batch_y = train_y[processed:processed+BATCH_SIZE]\n",
    "                processed += BATCH_SIZE\n",
    "\n",
    "                feed_dict = {tf_train_X:batch_X,\n",
    "                            tf_train_y:batch_y}\n",
    "\n",
    "                _, l, rl, pred, l2lw = sess.run([opt, total_loss, reg_loss, train_predictions, l2_lambda_weight], feed_dict=feed_dict)\n",
    "                total_steps += 1\n",
    "                \n",
    "                if total_steps % fl.l2_lambda_weight_decay_steps == 0:\n",
    "                    sess.run(l2_lambda_decay_op)\n",
    "                \n",
    "                #Validation Set\n",
    "                if total_steps % fl.validation_frequency == 0:\n",
    "                    feed_dict = {tf_validation_X:validation_X,\n",
    "                                 tf_validation_y:validation_y}\n",
    "                    pred_labels, true_labels = sess.run([validation_predictions, one_hot_validation_outputs], feed_dict=feed_dict)\n",
    "                    print(\"Validation Top-1 accuracy is \" + str(100.0*data_utils.n_accuracy(pred_labels, true_labels, 1)) + \"%\")\n",
    "        \n",
    "        #Test Set\n",
    "        feed_dict = {tf_test_X:test_X,tf_test_y:test_y}\n",
    "        pred_labels, true_labels = sess.run([test_predictions, one_hot_test_outputs], feed_dict=feed_dict)\n",
    "        print(\"Test Top-1 accuracy is \" + str(100.0*data_utils.n_accuracy(pred_labels, true_labels, 1)) + \"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating basic model\n",
      "flat pool_1_out shape is(64, 2028)\n",
      "unnormalized_logits shape is(64, 100)\n",
      "flat pool_1_out shape is(10000, 2028)\n",
      "unnormalized_logits shape is(10000, 100)\n",
      "flat pool_1_out shape is(10000, 2028)\n",
      "unnormalized_logits shape is(10000, 100)\n",
      "Validation Top-1 accuracy is 9.73%\n",
      "Validation Top-1 accuracy is 11.75%\n",
      "Validation Top-1 accuracy is 12.46%\n",
      "Validation Top-1 accuracy is 13.18%\n",
      "Validation Top-1 accuracy is 14.2%\n",
      "Validation Top-1 accuracy is 14.58%\n",
      "Validation Top-1 accuracy is 14.53%\n",
      "Validation Top-1 accuracy is 14.7%\n",
      "Validation Top-1 accuracy is 14.91%\n",
      "Validation Top-1 accuracy is 15.17%\n",
      "Validation Top-1 accuracy is 15.89%\n",
      "Validation Top-1 accuracy is 15.58%\n",
      "Validation Top-1 accuracy is 15.92%\n",
      "Validation Top-1 accuracy is 16.02%\n",
      "Validation Top-1 accuracy is 15.88%\n",
      "Validation Top-1 accuracy is 16.11%\n",
      "Validation Top-1 accuracy is 16.03%\n",
      "Validation Top-1 accuracy is 16.34%\n",
      "Validation Top-1 accuracy is 16.25%\n",
      "Validation Top-1 accuracy is 16.42%\n",
      "Validation Top-1 accuracy is 16.67%\n",
      "Validation Top-1 accuracy is 16.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8704ab6db118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_unit_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"classifier_basic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ead9485e06f7>\u001b[0m in \u001b[0;36mrun_unit_test\u001b[0;34m(use_fake_data, test_mode)\u001b[0m\n\u001b[1;32m    157\u001b[0m                             tf_train_y:batch_y}\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2lw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_lambda_weight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mtotal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_unit_test(use_fake_data=False, test_mode=\"classifier_basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
